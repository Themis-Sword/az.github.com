<!DOCTYPE html>
<!--[if lte IE 8 ]>
<html class="ie" xmlns="http://www.w3.org/1999/xhtml" xml:lang="en-US" lang="en-US">
<![endif]-->
<!--[if (gte IE 9)|!(IE)]><!-->
<!--
***************  *      *     *
      8          *    *       *
      8          *  *         *
      8          **           *
      8          *  *         *
      8          *    *       *
      8          *      *     *
      8          *        *   ***********    -----Theme By Kieran(http://go.kieran.top)
-->
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en-US" lang="en-US">
<!--<![endif]-->

<head>
  <title>Essentials of Machine Learning Algorithms with Python and R Codes (FW) | Themis_Sword&#39;s Blog</title>
  <!-- Meta data -->
    <meta http-equiv="Content-Type" content="text/html" charset="UTF-8" >
    <meta http-equiv="X-UA-Compatible" content="IE=edge"/>
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <meta name="generator" content="Themis_Sword's Blog">
    <meta name="author" content="Themis_Sword">
    <meta name="description" content="Reverence for Nature, and Life." />
    <meta name="keywords" content="" />

    <!-- Favicon, (keep icon in root folder) -->
    <link rel="Shortcut Icon" href="/img/favicon.ico" type="image/ico">

    <link rel="alternate" href="/atom.xml" title="Themis_Sword&#39;s Blog" type="application/atom+xml">
    <link rel="stylesheet" href="/css/all.css" media="screen" type="text/css">
	
    <link rel="stylesheet" href="/highlightjs/vs.css" type="text/css">
    
    

    <!-- Custom stylesheet, (add custom styles here, always load last) -->
    <!-- Load our stylesheet for IE8 -->
    <!--[if IE 8]>
    <link rel="stylesheet" type="text/css" href="/css/ie8.css" />
    <![endif]-->

    <!-- Google Webfonts (Monserrat 400/700, Open Sans 400/600) -->
    <link href='//fonts.useso.com/css?family=Montserrat:400,700' rel='stylesheet' type='text/css'>
    <link href='//fonts.useso.com/css?family=Open+Sans:400,600' rel='stylesheet' type='text/css'>

    <!-- Load our fonts individually if IE8+, to avoid faux bold & italic rendering -->
    <!--[if IE]>
    <link href='http://fonts.useso.com/css?family=Montserrat:400' rel='stylesheet' type='text/css'>
    <link href='http://fonts.useso.com/css?family=Montserrat:700' rel='stylesheet' type='text/css'>
    <link href='http://fonts.useso.com/css?family=Open+Sans:400' rel='stylesheet' type='text/css'>
    <link href='http://fonts.useso.com/css?family=Open+Sans:600' rel='stylesheet' type='text/css'>
    <![endif]-->

    <!-- jQuery | Load our jQuery, with an alternative source fallback to a local version if request is unavailable -->
    <script src="/js/jquery-1.11.1.min.js"></script>
    <script>window.jQuery || document.write('<script src="js/jquery-1.11.1.min.js"><\/script>')</script>

    <!-- Load these in the <head> for quicker IE8+ load times -->
    <!-- HTML5 shim and Respond.js IE8 support of HTML5 elements and media queries -->
    <!--[if lt IE 9]>
    <script src="/js/html5shiv.min.js"></script>
    <script src="/js/respond.min.js"></script>
    <![endif]-->










  
  
  

  
  <style>.col-md-8.col-md-offset-2.opening-statement img{display:none;}</style>
</head>

<!--
<body class="post-template">
-->
<body id="index" class="lightnav animsition">

      <!-- ============================ Off-canvas navigation =========================== -->

    <div class="sb-slidebar sb-right sb-style-overlay sb-momentum-scrolling">
        <div class="sb-close" aria-label="Close Menu" aria-hidden="true">
            <img src="/img/close.png" alt="Close"/>
        </div>
        <!-- Lists in Slidebars -->
        <ul class="sb-menu">
            <li><a href="/" class="animsition-link" title="Home">Home</a></li>
            
        	<li>
        		<a class="sb-toggle-submenu">Categories<span class="sb-caret"></span></a>
            	<ul class="sb-submenu">
				  	
				    <li><a href="/categories/artificial-intelligence/" class="animsition-link">artificial-intelligence<small>(10)</small></a></li>
				    
				    <li><a href="/categories/business/" class="animsition-link">business<small>(3)</small></a></li>
				    
				    <li><a href="/categories/inspiring/" class="animsition-link">inspiring<small>(2)</small></a></li>
				    
				    <li><a href="/categories/justice/" class="animsition-link">justice<small>(3)</small></a></li>
				    
				    <li><a href="/categories/mood/" class="animsition-link">mood<small>(18)</small></a></li>
				    
				    <li><a href="/categories/python/" class="animsition-link">python<small>(18)</small></a></li>
				    
				    <li><a href="/categories/python-artificial-intelligence/" class="animsition-link">python artificial-intelligence<small>(3)</small></a></li>
				    
				    <li><a href="/categories/software/" class="animsition-link">software<small>(3)</small></a></li>
				    
				    <li><a href="/categories/web/" class="animsition-link">web<small>(4)</small></a></li>
				    
				</ul>
        	</li>
			
            <li><a href="/information" class="animsition-link" title="Information">Information</a></li>
            <li><a href="/gallery" class="animsition-link" title="Gallery">Gallery</a></li>
            <li><a href="/timeline" class="animsition-link" title="Timeline">Timeline</a></li>
        </ul>
        <!-- Lists in Slidebars -->
        <ul class="sb-menu">
            <li><a href="/author" class="animsition-link" title="Author">Author</a></li>
        </ul>
    </div>
    
    <!-- ============================ END Off-canvas navigation =========================== -->

    <!-- ============================ #sb-site Main Page Wrapper =========================== -->

    <div id="sb-site">
        <!-- #sb-site - All page content should be contained within this id, except the off-canvas navigation itself -->

        <!-- ============================ Header & Logo bar =========================== -->

        <div id="navigation" class="navbar navbar-fixed-top">
            <div class="navbar-inner">
                <div class="container">
                    <!-- Nav logo -->
                    <div class="logo">
                        <a href="/" title="Logo" class="animsition-link">
                         <img src="/img/logo.png" alt="Logo" width="35px;"/> 
                        </a>
                    </div>
                    <!-- // Nav logo -->
                    <!-- Info-bar -->
                    <nav>
                        <ul class="nav">
                            <li><a href="/" class="animsition-link">Themis_Sword's Blog</a></li>
                            <li class="nolink">Reverence for Nature, and Life.</li>
                            
                            
                            
                            
                            
                        </ul>
                    </nav>
                    <!--// Info-bar -->
                </div>
                <!-- // .container -->
                <div class="learnmore sb-toggle-right">More</div>
                <button type="button" class="navbar-toggle menu-icon sb-toggle-right" title="More">
                <span class="sr-only">Toggle navigation</span>
                <span class="icon-bar before"></span>
                <span class="icon-bar main"></span>
                <span class="icon-bar after"></span>
                </button>
            </div>
            <!-- // .navbar-inner -->
        </div>

        <!-- ============================ Header & Logo bar =========================== -->

        <!-- ============================ Hero Image =========================== -->

        <section id="hero" class="scrollme">
            <div class="container-fluid element-img" style="background: url(/img/1.jpg) no-repeat center center fixed;background-size: cover">
                <div class="row">
                    <div class="col-xs-12 col-sm-8 col-sm-offset-2 col-md-8 col-md-offset-2 vertical-align cover boost text-center">
                        <div class="center-me animateme" data-when="exit" data-from="0" data-to="0.6" data-opacity="0" data-translatey="100">
                            <div>
                            	
                                <h2><span>Love, and to be Loved.</span></h2>
                                <p></p>
				    			
                                <h2></h2>
                                <p>我愛你，你是自由的。</p>
				    			

                            </div>
                        </div>
                    </div>
                    <!-- // .col-md-12 -->
                </div>
                <div class="herofade beige-dk"></div>
            </div>
        </section>

        <!-- Height spacing helper -->
        <div class="heightblock"></div>
        <!-- // End height spacing helper -->

        <!-- ============================ END Hero Image =========================== -->
      
<section id="intro">
    <div class="container">
        <div class="row col-md-offset-2">
            <div class="col-md-8">
    			<span class="post-meta">
      <time datetime="2016-07-25T04:50:10.000Z" itemprop="datePublished">
          2016-07-25
      </time>
    
    
    | 
    <a href='/categories/artificial-intelligence/'>artificial-intelligence</a>
    
    
</span>
                <h1>Essentials of Machine Learning Algorithms with Python and R Codes (FW)</h1>
            </div>
        </div>
        <div class="col-md-8 col-md-offset-2">
      		<h3 id="Introduction">Introduction</h3><blockquote>
<p><em>Google’s self-driving cars and robots get a lot of press, but the company’s real future is in machine learning, the technology that enables computers to get smarter and more personal.</em><br><strong>– Eric Schmidt (Google Chairman)</strong></p>
</blockquote>
<p>We are probably living in the most defining period of human history. The period when computing moved from large mainframes to PCs to cloud. But what makes it defining is not what has happened, but what is coming our way in years to come.<a id="more"></a>  </p>
<p>What makes this period exciting for some one like me is the democratization of the tools and techniques, which followed the boost in computing. Today, as a data scientist, I can build data crunching machines with complex algorithms for a few dollors per hour. But, reaching here wasn’t easy! I had my dark days and nights.  </p>
<h3 id="Who_can_benefit_the_most_from_this_guide?">Who can benefit the most from this guide?</h3><h4 id="What_I_am_giving_out_today_is_probably_the_most_valuable_guide,_I_have_ever_created-">What I am giving out today is probably the most valuable guide, I have ever created.</h4><p>The idea behind creating this guide is to simplify the journey of aspiring data scientists and machine learning enthusiasts across the world. Through this guide, I will enable you to work on machine learning problems and gain from experience. <strong>I am providing a high level understanding about various machine learning algorithms along with R &amp; Python codes to run them. These should be sufficient to get your hands dirty.</strong></p>
<img src="/images/Essentials1.jpg">  
<p>I have deliberately skipped the statistics behind these techniques, as you don’t need to understand them at the start. So, if you are looking for statistical understanding of these algorithms, you should look elsewhere. But, if you are looking to equip yourself to start building machine learning project, you are in for a treat.</p>
<h3 id="Broadly,_there_are_3_types_of_Machine_Learning_Algorithms-">Broadly, there are 3 types of Machine Learning Algorithms..</h3><h4 id="1-_Supervised_Learning">1. Supervised Learning</h4><p><strong><em>How it works</em></strong>: This algorithm consist of a target / outcome variable (or dependent variable) which is to be predicted from a given set of predictors (independent variables). Using these set of variables, we generate a function that map inputs to desired outputs. The training process continues until the model achieves a desired level of accuracy on the training data. Examples of Supervised Learning: Regression, <a href="http://www.analyticsvidhya.com/blog/2015/01/decision-tree-simplified/" target="_blank" rel="external">Decision Tree</a>, <a href="http://www.analyticsvidhya.com/blog/2014/06/introduction-random-forest-simplified/" target="_blank" rel="external">Random Forest</a>, KNN, Logistic Regression etc.</p>
<h4 id="2-_Unsupervised_Learning">2. Unsupervised Learning</h4><p><strong><em>How it works</em></strong>: In this algorithm, we do not have any target or outcome variable to predict / estimate.  It is used for clustering population in different groups, which is widely used for segmenting customers in different groups for specific intervention. Examples of Unsupervised Learning: Apriori algorithm, K-means.</p>
<h4 id="3-_Reinforcement_Learning:">3. Reinforcement Learning:</h4><p><strong><em>How it works</em></strong>:  Using this algorithm, the machine is trained to make specific decisions. It works this way: the machine is exposed to an environment where it trains itself continually using trial and error. This machine learns from past experience and tries to capture the best possible knowledge to make accurate business decisions. Example of Reinforcement Learning: Markov Decision Process</p>
<h3 id="List_of_Common_Machine_Learning_Algorithms">List of Common Machine Learning Algorithms</h3><p>Here is the list of commonly used machine learning algorithms. These algorithms can be applied to almost any data problem:</p>
<ol>
<li>Linear Regression  </li>
<li>Logistic Regression  </li>
<li>Decision Tree  </li>
<li>SVM  </li>
<li>Naive Bayes  </li>
<li>KNN  </li>
<li>K-Means  </li>
<li>Random Forest  </li>
<li>Dimensionality Reduction Algorithms  </li>
<li>Gradient Boost &amp; Adaboost  </li>
</ol>
<h3 id="1-_Linear_Regression">1. Linear Regression</h3><p>It is used to estimate real values (cost of houses, number of calls, total sales etc.) based on continuous variable(s). Here, we establish relationship between independent and dependent variables by fitting a best line. This best fit line is known as regression line and represented by a linear equation Y= a *X + b.</p>
<p>The best way to understand linear regression is to relive this experience of childhood. Let us say, you ask a child in fifth grade to arrange people in his class by increasing order of weight, without asking them their weights! What do you think the child will do? He / she would likely look (visually analyze) at the height and build of people and arrange them using a combination of these visible parameters. This is linear regression in real life! The child has actually figured out that height and build would be correlated to the weight by a relationship, which looks like the equation above.</p>
<p>In this equation:</p>
<ul>
<li>Y – Dependent Variable  </li>
<li>a – Slope  </li>
<li>X – Independent variable  </li>
<li>b – Intercept  </li>
</ul>
<p>These coefficients a and b are derived based on minimizing the sum of squared difference of distance between data points and regression line.</p>
<p>Look at the below example. Here we have identified the best fit line having linear equation <strong>y=0.2811x+13.9</strong>. Now using this equation, we can find the weight, knowing the height of a person.<br><img src="/images/Essentials2.png">  </p>
<p>Linear Regression is of mainly two types: Simple Linear Regression and Multiple Linear Regression. Simple Linear Regression is characterized by one independent variable. And, Multiple Linear Regression(as the name suggests) is characterized by multiple (more than 1) independent variables. While finding best fit line, you can fit a polynomial or curvilinear regression. And these are known as polynomial or curvilinear regression.</p>
<p><strong><em>Python Code</em></strong>  </p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">#Import Library</span><br><span class="line">#Import other necessary libraries like pandas, numpy...</span><br><span class="line">from sklearn import linear_model</span><br><span class="line">#<span class="operator"><span class="keyword">Load</span> Train <span class="keyword">and</span> <span class="keyword">Test</span> datasets</span><br><span class="line">#Identify feature <span class="keyword">and</span> response <span class="keyword">variable</span>(s) <span class="keyword">and</span> <span class="keyword">values</span> must be <span class="built_in">numeric</span> <span class="keyword">and</span> numpy arrays</span><br><span class="line">x_train=input_variables_values_training_datasets</span><br><span class="line">y_train=target_variables_values_training_datasets</span><br><span class="line">x_test=input_variables_values_test_datasets</span><br><span class="line"># <span class="keyword">Create</span> linear regression <span class="keyword">object</span></span><br><span class="line">linear = linear_model.LinearRegression()</span><br><span class="line"># Train the <span class="keyword">model</span> <span class="keyword">using</span> the training <span class="keyword">sets</span> <span class="keyword">and</span> <span class="keyword">check</span> score</span><br><span class="line">linear.fit(x_train, y_train)</span><br><span class="line">linear.score(x_train, y_train)</span><br><span class="line">#Equation coefficient <span class="keyword">and</span> Intercept</span><br><span class="line">print(<span class="string">'Coefficient: \n'</span>, linear.coef_)</span><br><span class="line">print(<span class="string">'Intercept: \n'</span>, linear.intercept_)</span><br><span class="line">#Predict <span class="keyword">Output</span></span><br><span class="line">predicted= linear.predict(x_test)</span></span><br></pre></td></tr></table></figure>
<p><strong><em>R Code</em></strong>  </p>
<figure class="highlight livecodeserver"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#Load Train and Test datasets</span></span><br><span class="line"><span class="comment">#Identify feature and response variable(s) and values must be numeric and numpy arrays</span></span><br><span class="line">x_train &lt;- input_variables_values_training_datasets</span><br><span class="line">y_train &lt;- target_variables_values_training_datasets</span><br><span class="line">x_test &lt;- input_variables_values_test_datasets</span><br><span class="line">x &lt;- cbind(x_train,y_train)</span><br><span class="line"><span class="comment"># Train the model using the training sets and check score</span></span><br><span class="line">linear &lt;- lm(y_train ~ ., data = x)</span><br><span class="line">summary(linear)</span><br><span class="line"><span class="comment">#Predict Output</span></span><br><span class="line">predicted= predict(linear,x_test) </span><br><span class="line">```   </span><br><span class="line"></span><br><span class="line"><span class="comment">### 2. Logistic Regression</span></span><br><span class="line"></span><br><span class="line">Don’t <span class="built_in">get</span> confused <span class="keyword">by</span> its name! It is <span class="operator">a</span> classification <span class="operator">not</span> <span class="operator">a</span> regression algorithm. It is used <span class="built_in">to</span> estimate discrete values ( Binary values like <span class="number">0</span>/<span class="number">1</span>, yes/no, <span class="constant">true</span>/<span class="constant">false</span> ) based <span class="command"><span class="keyword">on</span> <span class="title">given</span> <span class="title">set</span> <span class="title">of</span> <span class="title">independent</span> <span class="title">variable</span>(<span class="title">s</span>). <span class="title">In</span> <span class="title">simple</span> <span class="title">words</span>, <span class="title">it</span> <span class="title">predicts</span> <span class="title">the</span> <span class="title">probability</span> <span class="title">of</span> <span class="title">occurrence</span> <span class="title">of</span> <span class="title">an</span> <span class="title">event</span> <span class="title">by</span> <span class="title">fitting</span> <span class="title">data</span> <span class="title">to</span> <span class="title">a</span> [<span class="title">logit</span> <span class="title">function</span>](<span class="title">https</span>://<span class="title">en</span>.<span class="title">wikipedia</span>.<span class="title">org</span>/<span class="title">wiki</span>/<span class="title">Logistic_function</span>). <span class="title">Hence</span>, <span class="title">it</span> <span class="title">is</span> <span class="title">also</span> <span class="title">known</span> <span class="title">as</span> **<span class="title">logit</span> <span class="title">regression</span>**. <span class="title">Since</span>, <span class="title">it</span> <span class="title">predicts</span> <span class="title">the</span> <span class="title">probability</span>, <span class="title">its</span> <span class="title">output</span> <span class="title">values</span> <span class="title">lies</span> <span class="title">between</span> <span class="title">0</span> <span class="title">and</span> <span class="title">1</span> (<span class="title">as</span> <span class="title">expected</span>).</span></span><br><span class="line"></span><br><span class="line">Again, let us <span class="keyword">try</span> <span class="operator">and</span> understand this through <span class="operator">a</span> simple example.</span><br><span class="line"></span><br><span class="line">Let’s say your friend gives you <span class="operator">a</span> puzzle <span class="built_in">to</span> solve. There are only <span class="number">2</span> outcome scenarios – either you solve <span class="keyword">it</span> <span class="operator">or</span> you don’t. Now imagine, that you are being given wide range <span class="operator">of</span> puzzles / quizzes <span class="operator">in</span> <span class="operator">an</span> attempt <span class="built_in">to</span> understand which subjects you are good <span class="keyword">at</span>. The outcome <span class="built_in">to</span> this study would be something like this – <span class="keyword">if</span> you are given <span class="operator">a</span> trignometry based <span class="keyword">tenth</span> grade problem, you are <span class="number">70</span>% likely <span class="built_in">to</span> solve <span class="keyword">it</span>. On <span class="operator">the</span> other hand, <span class="keyword">if</span> <span class="keyword">it</span> is grade <span class="keyword">fifth</span> history question, <span class="operator">the</span> probability <span class="operator">of</span> getting <span class="operator">an</span> answer is only <span class="number">30</span>%. This is what Logistic Regression provides you.</span><br><span class="line"></span><br><span class="line">Coming <span class="built_in">to</span> <span class="operator">the</span> math, <span class="operator">the</span> <span class="built_in">log</span> odds <span class="operator">of</span> <span class="operator">the</span> outcome is modeled <span class="keyword">as</span> <span class="operator">a</span> linear combination <span class="operator">of</span> <span class="operator">the</span> predictor variables.</span><br></pre></td></tr></table></figure>
<p>odds= p/ (1-p) = probability of event occurrence / probability of not event occurrence<br>ln(odds) = ln(p/(1-p))<br>logit(p) = ln(p/(1-p)) = b0+b1X1+b2X2+b3X3….+bkXk<br><figure class="highlight livecodeserver"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">Above, p is <span class="operator">the</span> probability <span class="operator">of</span> presence <span class="operator">of</span> <span class="operator">the</span> characteristic <span class="operator">of</span> interest. It chooses parameters that maximize <span class="operator">the</span> likelihood <span class="operator">of</span> observing <span class="operator">the</span> sample values rather than that minimize <span class="operator">the</span> <span class="built_in">sum</span> <span class="operator">of</span> squared errors (like <span class="operator">in</span> ordinary regression).</span><br><span class="line"></span><br><span class="line">Now, you may ask, why take <span class="operator">a</span> <span class="built_in">log</span>? For <span class="operator">the</span> sake <span class="operator">of</span> simplicity, let’s just say that this is <span class="constant">one</span> <span class="operator">of</span> <span class="operator">the</span> best mathematical way <span class="built_in">to</span> replicate <span class="operator">a</span> step <span class="function"><span class="keyword">function</span>. <span class="title">I</span> <span class="title">can</span> <span class="title">go</span> <span class="title">in</span> <span class="title">more</span> <span class="title">details</span>, <span class="title">but</span> <span class="title">that</span> <span class="title">will</span> <span class="title">beat</span> <span class="title">the</span> <span class="title">purpose</span> <span class="title">of</span> <span class="title">this</span> <span class="title">article</span>.</span></span><br><span class="line"></span><br><span class="line">&#123;% img /images/Essentials3.png %&#125;  </span><br><span class="line"></span><br><span class="line">***Python Code***</span><br></pre></td></tr></table></figure></p>
<p>#Import Library<br>from sklearn.linear_model import LogisticRegression</p>
<p>#Assumed you have, X (predictor) and Y (target) for training data set and x_test(predictor) of test_dataset</p>
<h1 id="Create_logistic_regression_object">Create logistic regression object</h1><p>model = LogisticRegression()</p>
<h1 id="Train_the_model_using_the_training_sets_and_check_score">Train the model using the training sets and check score</h1><p>model.fit(X, y)<br>model.score(X, y)</p>
<p>#Equation coefficient and Intercept<br>print(‘Coefficient: \n’, model.coef<em>)<br>print(‘Intercept: \n’, model.intercept</em>)</p>
<p>#Predict Output<br>predicted= model.predict(x_test)<br><figure class="highlight gherkin"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span>R Code<span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span></span><br></pre></td></tr></table></figure></p>
<p>x &lt;- cbind(x_train,y_train)</p>
<h1 id="Train_the_model_using_the_training_sets_and_check_score-1">Train the model using the training sets and check score</h1><p>logistic &lt;- glm(y_train ~ ., data = x,family=’binomial’)<br>summary(logistic)</p>
<p>#Predict Output<br>predicted= predict(logistic,x_test)<br><figure class="highlight oxygene"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">#### Furthermore..</span><br><span class="line"></span><br><span class="line">There are many different steps that could be tried <span class="keyword">in</span> <span class="keyword">order</span> <span class="keyword">to</span> improve the model:</span><br><span class="line"></span><br><span class="line">* including interaction terms</span><br><span class="line">* removing features</span><br><span class="line">* [regularization techniques](http:<span class="comment">//www.analyticsvidhya.com/blog/2015/02/avoid-over-fitting-regularization/)</span></span><br><span class="line">* <span class="keyword">using</span> a non-linear model</span><br><span class="line">  </span><br><span class="line">### <span class="number">3</span>. Decision Tree</span><br><span class="line"></span><br><span class="line">This <span class="keyword">is</span> one <span class="keyword">of</span> my favorite algorithm <span class="keyword">and</span> I use it quite frequently. It <span class="keyword">is</span> a <span class="keyword">type</span> <span class="keyword">of</span> supervised learning algorithm that <span class="keyword">is</span> mostly used <span class="keyword">for</span> classification problems. Surprisingly, it works <span class="keyword">for</span> both categorical <span class="keyword">and</span> continuous dependent variables. <span class="keyword">In</span> this algorithm, we split the population <span class="keyword">into</span> two <span class="keyword">or</span> more homogeneous sets. This <span class="keyword">is</span> done based <span class="keyword">on</span> most significant attributes/ independent variables <span class="keyword">to</span> make <span class="keyword">as</span> <span class="keyword">distinct</span> groups <span class="keyword">as</span> possible. <span class="keyword">For</span> more details, you can <span class="keyword">read</span>: [Decision Tree Simplified](http:<span class="comment">//www.analyticsvidhya.com/blog/2015/01/decision-tree-simplified/).</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&#123;% img /images/Essentials4.png %&#125;</span>  </span><br><span class="line">*source: [statsexchange](http:<span class="comment">//stats.stackexchange.com/)*</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">In</span> the image above, you can see that population <span class="keyword">is</span> classified <span class="keyword">into</span> four different groups based <span class="keyword">on</span> multiple attributes <span class="keyword">to</span> identify ‘<span class="keyword">if</span> they will play <span class="keyword">or</span> <span class="keyword">not</span>’. <span class="keyword">To</span> split the population <span class="keyword">into</span> different heterogeneous groups, it <span class="keyword">uses</span> various techniques like Gini, Information Gain, Chi-square, entropy.</span><br><span class="line"></span><br><span class="line">The best way <span class="keyword">to</span> understand how decision tree works, <span class="keyword">is</span> <span class="keyword">to</span> play Jezzball – a classic game <span class="keyword">from</span> Microsoft (image below). Essentially, you have a room <span class="keyword">with</span> moving walls <span class="keyword">and</span> you need <span class="keyword">to</span> <span class="keyword">create</span> walls such that maximum area gets cleared off <span class="keyword">with</span> <span class="keyword">out</span> the balls.</span><br><span class="line"></span><br><span class="line"><span class="comment">&#123;% img /images/Essentials5.jpg %&#125;</span>  </span><br><span class="line"></span><br><span class="line">So, every time you split the room <span class="keyword">with</span> a wall, you are trying <span class="keyword">to</span> <span class="keyword">create</span> <span class="number">2</span> different populations <span class="keyword">with</span> <span class="keyword">in</span> the same room. Decision trees work <span class="keyword">in</span> very similar fashion <span class="keyword">by</span> dividing a population <span class="keyword">in</span> <span class="keyword">as</span> different groups <span class="keyword">as</span> possible.</span><br><span class="line"></span><br><span class="line">*More: [Simplified Version <span class="keyword">of</span> Decision Tree Algorithms](http:<span class="comment">//www.analyticsvidhya.com/blog/2015/01/decision-tree-simplified/)*</span></span><br><span class="line"></span><br><span class="line">***Python Code***</span><br></pre></td></tr></table></figure></p>
<p>#Import Library</p>
<p>#Import other necessary libraries like pandas, numpy…<br>from sklearn import tree</p>
<p>#Assumed you have, X (predictor) and Y (target) for training data set and x_test(predictor) of test_dataset</p>
<h1 id="Create_tree_object">Create tree object</h1><p>model = tree.DecisionTreeClassifier(criterion=’gini’) # for classification, here you can change the algorithm as gini or entropy (information gain) by default it is gini  </p>
<h1 id="model_=_tree-DecisionTreeRegressor()_for_regression">model = tree.DecisionTreeRegressor() for regression</h1><h1 id="Train_the_model_using_the_training_sets_and_check_score-2">Train the model using the training sets and check score</h1><p>model.fit(X, y)<br>model.score(X, y)</p>
<p>#Predict Output<br>predicted= model.predict(x_test)<br><figure class="highlight gherkin"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span>R Code<span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span></span><br></pre></td></tr></table></figure></p>
<p>library(rpart)<br>x &lt;- cbind(x_train,y_train)</p>
<h1 id="grow_tree">grow tree</h1><p>fit &lt;- rpart(y_train ~ ., data = x,method=”class”)<br>summary(fit)</p>
<p>#Predict Output<br>predicted= predict(fit,x_test)<br><figure class="highlight livecodeserver"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment">### 4. SVM (Support Vector Machine)</span></span><br><span class="line"></span><br><span class="line">It is <span class="operator">a</span> classification method. In this algorithm, we plot <span class="keyword">each</span> data <span class="keyword">item</span> <span class="keyword">as</span> <span class="operator">a</span> point <span class="operator">in</span> n-dimensional <span class="constant">space</span> (where n is <span class="built_in">number</span> <span class="operator">of</span> features you have) <span class="operator">with</span> <span class="operator">the</span> <span class="built_in">value</span> <span class="operator">of</span> <span class="keyword">each</span> feature being <span class="operator">the</span> <span class="built_in">value</span> <span class="operator">of</span> <span class="operator">a</span> particular coordinate.</span><br><span class="line"></span><br><span class="line">For example, <span class="keyword">if</span> we only had <span class="constant">two</span> features like Height <span class="operator">and</span> Hair <span class="built_in">length</span> <span class="operator">of</span> <span class="operator">an</span> individual, we’d <span class="keyword">first</span> plot these <span class="constant">two</span> variables <span class="operator">in</span> <span class="constant">two</span> dimensional <span class="constant">space</span> where <span class="keyword">each</span> point has <span class="constant">two</span> co-ordinates (these co-ordinates are known <span class="keyword">as</span> **Support Vectors**)</span><br><span class="line"></span><br><span class="line">&#123;% img /images/Essentials6.png %&#125;  </span><br><span class="line"></span><br><span class="line">Now, we will find some <span class="built_in">line</span> that splits <span class="operator">the</span> data between <span class="operator">the</span> <span class="constant">two</span> differently classified groups <span class="operator">of</span> data. This will be <span class="operator">the</span> <span class="built_in">line</span> such that <span class="operator">the</span> distances <span class="built_in">from</span> <span class="operator">the</span> closest point <span class="operator">in</span> <span class="keyword">each</span> <span class="operator">of</span> <span class="operator">the</span> <span class="constant">two</span> groups will be farthest away.</span><br><span class="line"></span><br><span class="line">&#123;% img /images/Essentials7.png %&#125;  </span><br><span class="line"></span><br><span class="line">In <span class="operator">the</span> example shown above, <span class="operator">the</span> <span class="built_in">line</span> which splits <span class="operator">the</span> data <span class="keyword">into</span> <span class="constant">two</span> differently classified groups is <span class="operator">the</span> <span class="keyword">black</span> <span class="built_in">line</span>, since <span class="operator">the</span> <span class="constant">two</span> closest points are <span class="operator">the</span> farthest apart <span class="built_in">from</span> <span class="operator">the</span> <span class="built_in">line</span>. This <span class="built_in">line</span> is our classifier. Then, depending <span class="command"><span class="keyword">on</span> <span class="title">where</span> <span class="title">the</span> <span class="title">testing</span> <span class="title">data</span> <span class="title">lands</span> <span class="title">on</span> <span class="title">either</span> <span class="title">side</span> <span class="title">of</span> <span class="title">the</span> <span class="title">line</span>, <span class="title">that</span>’<span class="title">s</span> <span class="title">what</span> <span class="title">class</span> <span class="title">we</span> <span class="title">can</span> <span class="title">classify</span> <span class="title">the</span> <span class="title">new</span> <span class="title">data</span> <span class="title">as</span>.</span></span><br><span class="line"></span><br><span class="line">*More: [Simplified Version <span class="operator">of</span> Support Vector Machine](<span class="keyword">http</span>://www.analyticsvidhya.com/blog/<span class="number">2014</span>/<span class="number">10</span>/support-vector-machine-simplified/)*</span><br><span class="line"></span><br><span class="line">**Think <span class="operator">of</span> this algorithm <span class="keyword">as</span> playing JezzBall <span class="operator">in</span> n-dimensional <span class="constant">space</span>. The tweaks <span class="operator">in</span> <span class="operator">the</span> game are:**</span><br><span class="line"></span><br><span class="line">* You can draw <span class="keyword">lines</span> / planes <span class="keyword">at</span> <span class="keyword">any</span> angles (rather than just horizontal <span class="operator">or</span> vertical <span class="keyword">as</span> <span class="operator">in</span> classic game)</span><br><span class="line">* The objective <span class="operator">of</span> <span class="operator">the</span> game is <span class="built_in">to</span> segregate balls <span class="operator">of</span> different colors <span class="operator">in</span> different rooms.</span><br><span class="line">* And <span class="operator">the</span> balls are <span class="operator">not</span> moving.</span><br><span class="line">  </span><br><span class="line">***Python Code***</span><br></pre></td></tr></table></figure></p>
<p>#Import Library<br>from sklearn import svm</p>
<p>#Assumed you have, X (predictor) and Y (target) for training data set and x_test(predictor) of test_dataset</p>
<h1 id="Create_SVM_classification_object">Create SVM classification object</h1><p>model = svm.svc() # there is various option associated with it, this is simple for classification. You can refer link, for mo# re detail.</p>
<h1 id="Train_the_model_using_the_training_sets_and_check_score-3">Train the model using the training sets and check score</h1><p>model.fit(X, y)<br>model.score(X, y)</p>
<p>#Predict Output<br>predicted= model.predict(x_test)<br><figure class="highlight gherkin"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span>R Code<span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span></span><br></pre></td></tr></table></figure></p>
<p>library(e1071)<br>x &lt;- cbind(x_train,y_train)</p>
<h1 id="Fitting_model">Fitting model</h1><p>fit &lt;-svm(y_train ~ ., data = x)<br>summary(fit)</p>
<p>#Predict Output<br>predicted= predict(fit,x_test)<br><figure class="highlight oxygene"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">### <span class="number">5</span>. Naive Bayes</span><br><span class="line"></span><br><span class="line">It <span class="keyword">is</span> a classification technique based <span class="keyword">on</span> [Bayes’ theorem](https:<span class="comment">//en.wikipedia.org/wiki/Bayes%27_theorem) with an assumption of independence between predictors. In simple terms, a Naive Bayes classifier assumes that the presence of a particular feature in a class is unrelated to the presence of any other feature. For example, a fruit may be considered to be an apple if it is red, round, and about 3 inches in diameter. Even if these features depend on each other or upon the existence of the other features, a naive Bayes classifier would consider all of these properties to independently contribute to the probability that this fruit is an apple.</span></span><br><span class="line"></span><br><span class="line">Naive Bayesian model <span class="keyword">is</span> easy <span class="keyword">to</span> build <span class="keyword">and</span> particularly useful <span class="keyword">for</span> very large data sets. Along <span class="keyword">with</span> simplicity, Naive Bayes <span class="keyword">is</span> known <span class="keyword">to</span> outperform even highly sophisticated classification methods.</span><br><span class="line"></span><br><span class="line">Bayes theorem provides a way <span class="keyword">of</span> calculating posterior probability P(c|x) <span class="keyword">from</span> P(c), P(x) <span class="keyword">and</span> P(x|c). Look at the equation below:</span><br><span class="line"><span class="comment">&#123;% img /images/Essentials8.png %&#125;</span>  </span><br><span class="line"></span><br><span class="line">Here,</span><br><span class="line"></span><br><span class="line">* P(c|x) <span class="keyword">is</span> the posterior probability <span class="keyword">of</span> <span class="keyword">class</span> (target) given predictor (attribute). </span><br><span class="line">* P(c) <span class="keyword">is</span> the prior probability <span class="keyword">of</span> <span class="keyword">class</span>. </span><br><span class="line">* P(x|c) <span class="keyword">is</span> the likelihood which <span class="keyword">is</span> the probability <span class="keyword">of</span> predictor given <span class="keyword">class</span>. </span><br><span class="line">* P(x) <span class="keyword">is</span> the prior probability <span class="keyword">of</span> predictor.</span><br><span class="line"></span><br><span class="line">**Example**: Let’s understand it <span class="keyword">using</span> an example. Below I have a training data <span class="keyword">set</span> <span class="keyword">of</span> weather <span class="keyword">and</span> corresponding target variable ‘Play’. Now, we need <span class="keyword">to</span> classify whether players will play <span class="keyword">or</span> <span class="keyword">not</span> based <span class="keyword">on</span> weather condition. Let’s follow the below steps <span class="keyword">to</span> perform it.</span><br><span class="line"></span><br><span class="line"><span class="keyword">Step</span> <span class="number">1</span>: Convert the data <span class="keyword">set</span> <span class="keyword">to</span> frequency table</span><br><span class="line"></span><br><span class="line"><span class="keyword">Step</span> <span class="number">2</span>: <span class="keyword">Create</span> Likelihood table <span class="keyword">by</span> finding the probabilities like Overcast probability = <span class="number">0.29</span> <span class="keyword">and</span> probability <span class="keyword">of</span> playing <span class="keyword">is</span> <span class="number">0.64</span>.</span><br><span class="line"></span><br><span class="line"><span class="comment">&#123;% img /images/Essentials9.png %&#125;</span>  </span><br><span class="line"></span><br><span class="line"><span class="keyword">Step</span> <span class="number">3</span>: Now, use Naive Bayesian equation <span class="keyword">to</span> calculate the posterior probability <span class="keyword">for</span> <span class="keyword">each</span> <span class="keyword">class</span>. The <span class="keyword">class</span> <span class="keyword">with</span> the highest posterior probability <span class="keyword">is</span> the outcome <span class="keyword">of</span> prediction.</span><br><span class="line"></span><br><span class="line">**Problem**: Players will pay <span class="keyword">if</span> weather <span class="keyword">is</span> sunny, <span class="keyword">is</span> this statement <span class="keyword">is</span> correct?</span><br><span class="line"></span><br><span class="line">We can solve it <span class="keyword">using</span> above discussed <span class="function"><span class="keyword">method</span>, <span class="title">so</span> <span class="title">P</span><span class="params">(Yes | Sunny)</span> = <span class="title">P</span><span class="params">( Sunny | Yes)</span> * <span class="title">P</span><span class="params">(Yes)</span> / <span class="title">P</span> <span class="params">(Sunny)</span></span><br><span class="line"></span><br><span class="line"><span class="title">Here</span> <span class="title">we</span> <span class="title">have</span> <span class="title">P</span> <span class="params">(Sunny |Yes)</span> = 3/9 = 0.33, <span class="title">P</span><span class="params">(Sunny)</span> = 5/14 = 0.36, <span class="title">P</span><span class="params">( Yes)</span>= 9/14 = 0.64</span><br><span class="line"></span><br><span class="line"><span class="title">Now</span>, <span class="title">P</span> <span class="params">(Yes | Sunny)</span> = 0.33 * 0.64 / 0.36 = 0.60, <span class="title">which</span> <span class="title">has</span> <span class="title">higher</span> <span class="title">probability</span>.</span><br><span class="line"></span><br><span class="line"><span class="title">Naive</span> <span class="title">Bayes</span> <span class="title">uses</span> <span class="title">a</span> <span class="title">similar</span> <span class="title">method</span> <span class="title">to</span> <span class="title">predict</span> <span class="title">the</span> <span class="title">probability</span> <span class="title">of</span> <span class="title">different</span> <span class="title">class</span> <span class="title">based</span> <span class="title">on</span> <span class="title">various</span> <span class="title">attributes</span>. <span class="title">This</span> <span class="title">algorithm</span> <span class="title">is</span> <span class="title">mostly</span> <span class="title">used</span> <span class="title">in</span> <span class="title">text</span> <span class="title">classification</span> <span class="title">and</span> <span class="title">with</span> <span class="title">problems</span> <span class="title">having</span> <span class="title">multiple</span> <span class="title">classes</span>.</span><br><span class="line"></span><br><span class="line">***<span class="title">Python</span> <span class="title">Code</span>***</span></span><br></pre></td></tr></table></figure></p>
<p>#Import Library<br>from sklearn.naive_bayes import GaussianNB</p>
<p>#Assumed you have, X (predictor) and Y (target) for training data set and x_test(predictor) of test_dataset</p>
<h1 id="Create_SVM_classification_object_model_=_GaussianNB()_#_there_is_other_distribution_for_multinomial_classes_like_Bernoulli_Naive_Bayes,_Refer_link">Create SVM classification object model = GaussianNB() # there is other distribution for multinomial classes like Bernoulli Naive Bayes, Refer link</h1><h1 id="Train_the_model_using_the_training_sets_and_check_score-4">Train the model using the training sets and check score</h1><p>model.fit(X, y)</p>
<p>#Predict Output<br>predicted= model.predict(x_test)<br><figure class="highlight gherkin"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span>R Code<span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span></span><br></pre></td></tr></table></figure></p>
<p>library(e1071)<br>x &lt;- cbind(x_train,y_train)</p>
<h1 id="Fitting_model-1">Fitting model</h1><p>fit &lt;-naiveBayes(y_train ~ ., data = x)<br>summary(fit)</p>
<p>#Predict Output<br>predicted= predict(fit,x_test)<br><figure class="highlight applescript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment">### 6. KNN (K- Nearest Neighbors)</span></span><br><span class="line"></span><br><span class="line">It can be used <span class="keyword">for</span> both classification <span class="keyword">and</span> regression problems. However, <span class="keyword">it</span> <span class="keyword">is</span> more widely used <span class="keyword">in</span> classification problems <span class="keyword">in</span> <span class="keyword">the</span> industry. K nearest neighbors <span class="keyword">is</span> a simple algorithm <span class="keyword">that</span> stores all available cases <span class="keyword">and</span> classifies new cases <span class="keyword">by</span> a majority vote <span class="keyword">of</span> <span class="keyword">its</span> k neighbors. The case being assigned <span class="keyword">to</span> <span class="keyword">the</span> <span class="type">class</span> <span class="keyword">is</span> most common amongst <span class="keyword">its</span> K nearest neighbors measured <span class="keyword">by</span> a distance function.</span><br><span class="line"></span><br><span class="line">These distance functions can be Euclidean, Manhattan, Minkowski <span class="keyword">and</span> Hamming distance. First three functions are used <span class="keyword">for</span> continuous function <span class="keyword">and</span> <span class="keyword">fourth</span> one (Hamming) <span class="keyword">for</span> categorical variables. If K = <span class="number">1</span>, <span class="keyword">then</span> <span class="keyword">the</span> case <span class="keyword">is</span> simply assigned <span class="keyword">to</span> <span class="keyword">the</span> <span class="type">class</span> <span class="keyword">of</span> <span class="keyword">its</span> nearest neighbor. At <span class="keyword">times</span>, choosing K turns out <span class="keyword">to</span> be a challenge <span class="keyword">while</span> performing KNN modeling.</span><br><span class="line"></span><br><span class="line">*More: Introduction <span class="keyword">to</span> k-nearest neighbors : Simplified.*</span><br><span class="line"></span><br><span class="line">&#123;% img /images/Essentials10.png %&#125;  </span><br><span class="line"></span><br><span class="line">KNN can easily be mapped <span class="keyword">to</span> our <span class="type">real</span> lives. If you want <span class="keyword">to</span> learn <span class="keyword">about</span> a person, <span class="keyword">of</span> whom you have no information, you might like <span class="keyword">to</span> find out <span class="keyword">about</span> his close friends <span class="keyword">and</span> <span class="keyword">the</span> circles he moves <span class="keyword">in</span> <span class="keyword">and</span> gain access <span class="keyword">to</span> his/her information!</span><br><span class="line"></span><br><span class="line">**Things <span class="keyword">to</span> consider <span class="keyword">before</span> selecting KNN:**  </span><br><span class="line"></span><br><span class="line">* KNN <span class="keyword">is</span> computationally expensive</span><br><span class="line">* Variables should be normalized <span class="keyword">else</span> higher range variables can bias <span class="keyword">it</span></span><br><span class="line">* Works <span class="function_start"><span class="keyword">on</span></span> pre-processing stage more <span class="keyword">before</span> going <span class="keyword">for</span> KNN like outlier, noise removal</span><br><span class="line"></span><br><span class="line">***Python Code***</span><br></pre></td></tr></table></figure></p>
<p>#Import Library<br>from sklearn.neighbors import KNeighborsClassifier</p>
<p>#Assumed you have, X (predictor) and Y (target) for training data set and x_test(predictor) of test_dataset</p>
<h1 id="Create_KNeighbors_classifier_object_model">Create KNeighbors classifier object model</h1><p>KNeighborsClassifier(n_neighbors=6) # default value for n_neighbors is 5</p>
<h1 id="Train_the_model_using_the_training_sets_and_check_score-5">Train the model using the training sets and check score</h1><p>model.fit(X, y)</p>
<p>#Predict Output<br>predicted= model.predict(x_test)<br><figure class="highlight gherkin"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span>R Code<span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span></span><br></pre></td></tr></table></figure></p>
<p>library(knn)<br>x &lt;- cbind(x_train,y_train)</p>
<h1 id="Fitting_model-2">Fitting model</h1><p>fit &lt;-knn(y_train ~ ., data = x,k=5)<br>summary(fit)</p>
<p>#Predict Output<br>predicted= predict(fit,x_test)<br><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">### 7. K-Means</span><br><span class="line"></span><br><span class="line">It is a type of unsupervised algorithm which  solves the clustering problem. Its procedure follows a simple and easy  way to classify a given data <span class="operator"><span class="keyword">set</span> <span class="keyword">through</span> a certain <span class="built_in">number</span> <span class="keyword">of</span>  clusters (assume <span class="keyword">k</span> clusters). <span class="keyword">Data</span> points inside a cluster <span class="keyword">are</span> homogeneous <span class="keyword">and</span> heterogeneous <span class="keyword">to</span> peer <span class="keyword">groups</span>.</span><br><span class="line"></span><br><span class="line">Remember figuring <span class="keyword">out</span> shapes <span class="keyword">from</span> ink blots? <span class="keyword">k</span> means <span class="keyword">is</span> somewhat similar this activity. You look <span class="keyword">at</span> the shape <span class="keyword">and</span> spread <span class="keyword">to</span> decipher how many different clusters / population <span class="keyword">are</span> <span class="keyword">present</span>!</span><br><span class="line"></span><br><span class="line">&#123;% img /images/Essentials11.jpg %&#125;  </span><br><span class="line"></span><br><span class="line">**How <span class="keyword">K</span>-means forms cluster:**</span><br><span class="line"></span><br><span class="line"><span class="number">1.</span> <span class="keyword">K</span>-means picks <span class="keyword">k</span> <span class="built_in">number</span> <span class="keyword">of</span> points <span class="keyword">for</span> <span class="keyword">each</span> cluster known <span class="keyword">as</span> centroids.</span><br><span class="line"><span class="number">2.</span> <span class="keyword">Each</span> <span class="keyword">data</span> point forms a cluster <span class="keyword">with</span> the closest centroids <span class="keyword">i</span>.<span class="keyword">e</span>. <span class="keyword">k</span> clusters.</span><br><span class="line"><span class="number">3.</span> Finds the centroid <span class="keyword">of</span> <span class="keyword">each</span> cluster based <span class="keyword">on</span> existing cluster members. Here we have <span class="keyword">new</span> centroids.</span><br><span class="line"><span class="number">4.</span> <span class="keyword">As</span> we have <span class="keyword">new</span> centroids, <span class="keyword">repeat</span> step <span class="number">2</span> <span class="keyword">and</span> <span class="number">3.</span> Find the closest distance <span class="keyword">for</span> <span class="keyword">each</span> <span class="keyword">data</span> point <span class="keyword">from</span> <span class="keyword">new</span> centroids <span class="keyword">and</span> <span class="keyword">get</span> associated <span class="keyword">with</span> <span class="keyword">new</span> <span class="keyword">k</span>-clusters. <span class="keyword">Repeat</span> this process <span class="keyword">until</span> convergence occurs <span class="keyword">i</span>.<span class="keyword">e</span>. centroids does <span class="keyword">not</span> <span class="keyword">change</span>.</span><br><span class="line"></span><br><span class="line">**How <span class="keyword">to</span> determine <span class="keyword">value</span> <span class="keyword">of</span> <span class="keyword">K</span>:**</span><br><span class="line"></span><br><span class="line"><span class="keyword">In</span> <span class="keyword">K</span>-means, we have clusters <span class="keyword">and</span> <span class="keyword">each</span> cluster has its own centroid. <span class="keyword">Sum</span> <span class="keyword">of</span> <span class="keyword">square</span> <span class="keyword">of</span> <span class="keyword">difference</span> <span class="keyword">between</span> centroid <span class="keyword">and</span> the <span class="keyword">data</span> points <span class="keyword">within</span> a cluster constitutes <span class="keyword">within</span> <span class="keyword">sum</span> <span class="keyword">of</span> <span class="keyword">square</span> <span class="keyword">value</span> <span class="keyword">for</span> that cluster. Also, <span class="keyword">when</span> the <span class="keyword">sum</span> <span class="keyword">of</span> <span class="keyword">square</span> <span class="keyword">values</span> <span class="keyword">for</span> all the clusters <span class="keyword">are</span> added, it becomes total <span class="keyword">within</span> <span class="keyword">sum</span> <span class="keyword">of</span> <span class="keyword">square</span> <span class="keyword">value</span> <span class="keyword">for</span> the cluster solution.</span><br><span class="line"></span><br><span class="line">We know that <span class="keyword">as</span> the <span class="built_in">number</span> <span class="keyword">of</span> cluster increases, this <span class="keyword">value</span> keeps <span class="keyword">on</span> decreasing but <span class="keyword">if</span> you plot the <span class="keyword">result</span> you may see that the <span class="keyword">sum</span> <span class="keyword">of</span> squared distance decreases sharply up <span class="keyword">to</span> <span class="keyword">some</span> <span class="keyword">value</span> <span class="keyword">of</span> <span class="keyword">k</span>, <span class="keyword">and</span> <span class="keyword">then</span> much more slowly <span class="keyword">after</span> that. Here, we can find the optimum <span class="built_in">number</span> <span class="keyword">of</span> cluster.</span><br><span class="line"></span><br><span class="line">&#123;% img /images/Essentials12.png %&#125;  </span><br><span class="line"></span><br><span class="line">***Python Code***</span></span><br></pre></td></tr></table></figure></p>
<p>#Import Library<br>from sklearn.cluster import KMeans</p>
<p>#Assumed you have, X (attributes) for training data set and x_test(attributes) of test_dataset</p>
<h1 id="Create_KNeighbors_classifier_object_model-1">Create KNeighbors classifier object model</h1><p>k_means = KMeans(n_clusters=3, random_state=0)</p>
<h1 id="Train_the_model_using_the_training_sets_and_check_score-6">Train the model using the training sets and check score</h1><p>model.fit(X)</p>
<p>#Predict Output<br>predicted= model.predict(x_test)<br><figure class="highlight gherkin"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span>R Code<span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span></span><br></pre></td></tr></table></figure></p>
<p>library(cluster)<br>fit &lt;- kmeans(X, 3) # 5 cluster solution<br><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">### 8. Random Forest</span><br><span class="line"></span><br><span class="line">Random Forest is a trademark term for an ensemble of decision trees. In Random Forest, we’ve collection of decision trees (so known as “Forest”). To classify a new object based on attributes, each tree gives a classification and we say the tree “votes” for that class. The forest chooses the classification having the most votes (over all the trees in the forest).</span><br><span class="line"></span><br><span class="line">Each tree is planted &amp; grown as follows:</span><br><span class="line"></span><br><span class="line">1. If the number of cases in the training <span class="operator"><span class="keyword">set</span> <span class="keyword">is</span> <span class="keyword">N</span>, <span class="keyword">then</span> <span class="keyword">sample</span> <span class="keyword">of</span> <span class="keyword">N</span> cases <span class="keyword">is</span> taken <span class="keyword">at</span> random but <span class="keyword">with</span> replacement. This <span class="keyword">sample</span> will be the training <span class="keyword">set</span> <span class="keyword">for</span> growing the tree.</span><br><span class="line"><span class="number">2.</span> <span class="keyword">If</span> there <span class="keyword">are</span> <span class="keyword">M</span> <span class="keyword">input</span> <span class="keyword">variables</span>, a <span class="built_in">number</span> <span class="keyword">m</span>&lt;&lt;<span class="keyword">M</span> <span class="keyword">is</span> specified such that <span class="keyword">at</span> <span class="keyword">each</span> node, <span class="keyword">m</span> <span class="keyword">variables</span> <span class="keyword">are</span> selected <span class="keyword">at</span> random <span class="keyword">out</span> <span class="keyword">of</span> the <span class="keyword">M</span> <span class="keyword">and</span> the best <span class="keyword">split</span> <span class="keyword">on</span> these <span class="keyword">m</span> <span class="keyword">is</span> used <span class="keyword">to</span> <span class="keyword">split</span> the node. The <span class="keyword">value</span> <span class="keyword">of</span> <span class="keyword">m</span> <span class="keyword">is</span> held <span class="keyword">constant</span> during the forest growing.</span><br><span class="line"><span class="number">3.</span> <span class="keyword">Each</span> tree <span class="keyword">is</span> grown <span class="keyword">to</span> the largest <span class="keyword">extent</span> possible. There <span class="keyword">is</span> <span class="keyword">no</span> pruning.</span><br><span class="line"><span class="keyword">For</span> more details <span class="keyword">on</span> this algorithm, comparing <span class="keyword">with</span> decision tree <span class="keyword">and</span> tuning <span class="keyword">model</span> <span class="keyword">parameters</span>, <span class="keyword">I</span> would suggest you <span class="keyword">to</span> <span class="keyword">read</span> these articles:</span><br><span class="line"></span><br><span class="line"><span class="number">1.</span> [Introduction <span class="keyword">to</span> Random forest – Simplified](<span class="keyword">http</span>://www.analyticsvidhya.com/blog/<span class="number">2014</span>/<span class="number">06</span>/introduction-random-forest-simplified/)</span><br><span class="line"></span><br><span class="line"><span class="number">2.</span> [Comparing a CART <span class="keyword">model</span> <span class="keyword">to</span> Random Forest (Part <span class="number">1</span>)](<span class="keyword">http</span>://www.analyticsvidhya.com/blog/<span class="number">2014</span>/<span class="number">06</span>/comparing-cart-random-forest-<span class="number">1</span>/)</span><br><span class="line"></span><br><span class="line"><span class="number">3.</span> [Comparing a Random Forest <span class="keyword">to</span> a CART <span class="keyword">model</span> (Part <span class="number">2</span>)](<span class="keyword">http</span>://www.analyticsvidhya.com/blog/<span class="number">2014</span>/<span class="number">06</span>/comparing-random-forest-simple-cart-<span class="keyword">model</span>/)</span><br><span class="line"></span><br><span class="line"><span class="number">4.</span> [Tuning the <span class="keyword">parameters</span> <span class="keyword">of</span> your Random Forest <span class="keyword">model</span>](<span class="keyword">http</span>://www.analyticsvidhya.com/blog/<span class="number">2015</span>/<span class="number">06</span>/tuning-random-forest-<span class="keyword">model</span>/)</span><br><span class="line"></span><br><span class="line">***Python Code***</span></span><br></pre></td></tr></table></figure></p>
<p>#Import Library<br>from sklearn.ensemble import RandomForestClassifier</p>
<p>#Assumed you have, X (predictor) and Y (target) for training data set and x_test(predictor) of test_dataset</p>
<h1 id="Create_Random_Forest_object">Create Random Forest object</h1><p>model= RandomForestClassifier()</p>
<h1 id="Train_the_model_using_the_training_sets_and_check_score-7">Train the model using the training sets and check score</h1><p>model.fit(X, y)</p>
<p>#Predict Output<br>predicted= model.predict(x_test)<br><figure class="highlight gherkin"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span>R Code<span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span></span><br></pre></td></tr></table></figure></p>
<p>library(randomForest)<br>x &lt;- cbind(x_train,y_train)</p>
<h1 id="Fitting_model-3">Fitting model</h1><p>fit &lt;- randomForest(Species ~ ., x,ntree=500)<br>summary(fit)</p>
<p>#Predict Output<br>predicted= predict(fit,x_test)<br><figure class="highlight livecodeserver"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment">### 9. Dimensionality Reduction Algorithms</span></span><br><span class="line"></span><br><span class="line">In <span class="operator">the</span> <span class="keyword">last</span> <span class="number">4</span>-<span class="number">5</span> years, there has been <span class="operator">an</span> exponential increase <span class="operator">in</span> data capturing <span class="keyword">at</span> every possible stages. Corporates/ Government Agencies/ Research organisations are <span class="operator">not</span> only coming <span class="operator">with</span> <span class="built_in">new</span> sources but also they are capturing data <span class="operator">in</span> great detail.</span><br><span class="line"></span><br><span class="line">For example: E-commerce companies are capturing more details about customer like their demographics, web crawling history, what they like <span class="operator">or</span> dislike, purchase history, feedback <span class="operator">and</span> many others <span class="built_in">to</span> give them personalized attention more than your nearest grocery shopkeeper.</span><br><span class="line"></span><br><span class="line">As <span class="operator">a</span> data scientist, <span class="operator">the</span> data we are offered also consist <span class="operator">of</span> many features, this sounds good <span class="keyword">for</span> building good robust model but there is <span class="operator">a</span> challenge. How’d you identify highly significant <span class="built_in">variable</span>(s) out <span class="number">1000</span> <span class="operator">or</span> <span class="number">2000</span>? In such cases, dimensionality reduction algorithm helps us along <span class="operator">with</span> various other algorithms like Decision Tree, Random Forest, PCA, Factor Analysis, Identify based <span class="command"><span class="keyword">on</span> <span class="title">correlation</span> <span class="title">matrix</span>, <span class="title">missing</span> <span class="title">value</span> <span class="title">ratio</span> <span class="title">and</span> <span class="title">others</span>.</span></span><br><span class="line"></span><br><span class="line">To know more about this algorithms, you can <span class="built_in">read</span> [“Beginners Guide To Learn Dimension Reduction Techniques“](<span class="keyword">http</span>://www.analyticsvidhya.com/blog/<span class="number">2015</span>/<span class="number">07</span>/dimension-reduction-methods/).</span><br><span class="line"></span><br><span class="line">***Python Code***</span><br></pre></td></tr></table></figure></p>
<p>#Import Library<br>from sklearn import decomposition</p>
<p>#Assumed you have training and test data set as train and test</p>
<h1 id="Create_PCA_obeject_pca=_decomposition-PCA(n_components=k)_#default_value_of_k_=min(n_sample,_n_features)">Create PCA obeject pca= decomposition.PCA(n_components=k) #default value of k =min(n_sample, n_features)</h1><h1 id="For_Factor_analysis">For Factor analysis</h1><p>#fa= decomposition.FactorAnalysis()</p>
<h1 id="Reduced_the_dimension_of_training_dataset_using_PCA">Reduced the dimension of training dataset using PCA</h1><p>train_reduced = pca.fit_transform(train)</p>
<p>#Reduced the dimension of test dataset<br>test_reduced = pca.transform(test)</p>
<p>#For more detail on this, please refer  this link.<br><figure class="highlight gherkin"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span>R Code<span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span></span><br></pre></td></tr></table></figure></p>
<p>library(stats)<br>pca &lt;- princomp(train, cor = TRUE)<br>train_reduced  &lt;- predict(pca,train)<br>test_reduced  &lt;- predict(pca,test)<br><figure class="highlight livecodeserver"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment">### 10. Gradient Boosting &amp; AdaBoost</span></span><br><span class="line"></span><br><span class="line">GBM &amp; AdaBoost are boosting algorithms used when we deal <span class="operator">with</span> plenty <span class="operator">of</span> data <span class="built_in">to</span> make <span class="operator">a</span> prediction <span class="operator">with</span> high prediction power. Boosting is <span class="operator">an</span> ensemble learning algorithm which combines <span class="operator">the</span> prediction <span class="operator">of</span> several base estimators <span class="operator">in</span> order <span class="built_in">to</span> improve robustness over <span class="operator">a</span> single estimator. It combines multiple weak <span class="operator">or</span> <span class="built_in">average</span> predictors <span class="built_in">to</span> <span class="operator">a</span> build strong predictor. These boosting algorithms always work well <span class="operator">in</span> data science competitions like Kaggle, AV Hackathon, CrowdAnalytix.</span><br><span class="line"></span><br><span class="line">*More: [Know about Gradient <span class="operator">and</span> AdaBoost <span class="operator">in</span> detail](<span class="keyword">http</span>://www.analyticsvidhya.com/blog/<span class="number">2015</span>/<span class="number">05</span>/boosting-algorithms-simplified/)*</span><br><span class="line"></span><br><span class="line">***Python Code***  </span><br><span class="line"></span><br><span class="line">```  </span><br><span class="line"><span class="comment">#Import Library</span></span><br><span class="line"><span class="built_in">from</span> sklearn.ensemble import GradientBoostingClassifier</span><br><span class="line"><span class="comment">#Assumed you have, X (predictor) and Y (target) for training data set and x_test(predictor) of test_dataset</span></span><br><span class="line"><span class="comment"># Create Gradient Boosting Classifier object</span></span><br><span class="line">model= GradientBoostingClassifier(n_estimators=<span class="number">100</span>, learning_rate=<span class="number">1.0</span>, max_depth=<span class="number">1</span>, random_state=<span class="number">0</span>)</span><br><span class="line"><span class="comment"># Train the model using the training sets and check score</span></span><br><span class="line">model.fit(X, y)</span><br><span class="line"><span class="comment">#Predict Output</span></span><br><span class="line">predicted= model.predict(x_test)</span><br><span class="line">```  </span><br><span class="line"></span><br><span class="line">***R Code***</span><br></pre></td></tr></table></figure></p>
<p>library(caret)<br>x &lt;- cbind(x_train,y_train)</p>
<h1 id="Fitting_model-4">Fitting model</h1><p>fitControl &lt;- trainControl( method = “repeatedcv”, number = 4, repeats = 4)<br>fit &lt;- train(y ~ ., data = x, method = “gbm”, trControl = fitControl,verbose = FALSE)<br>predicted= predict(fit,x_test,type= “prob”)[,2]<br>```  </p>
<p>GradientBoostingClassifier and Random Forest are two different boosting tree classifier and often people ask about the difference between these two algorithms.</p>
<h3 id="End_Notes">End Notes</h3><p>By now, I am sure, you would have an idea of commonly used machine learning algorithms. My sole intention behind writing this article and providing the codes in R and Python is to get you started right away. If you are keen to master machine learning, start right away. Take up problems, develop a physical understanding of the process, apply these codes and see the fun!</p>
<p><a href="http://www.analyticsvidhya.com/blog/2015/08/common-machine-learning-algorithms/" target="_blank" rel="external">Origin</a><br><a href="http://blog.jobbole.com/92021/" target="_blank" rel="external">中文譯文——10種機器學習算法的要點(附Python和R代碼)</a></p>

            <div class="clearfix"></div>
            <hr class="nogutter">
        </div>
        <nav class="pagination" role="pagination">
    
    
    <a class="pull-right" href="/blog/07122016/8個經過證實的方法：提高機器學習模型的準確率-轉/">
        8個經過證實的方法：提高機器學習模型的準確率 (轉) →
    </a>
    
</nav>

        <div class="duoshuo">
<div class="ds-thread" data-thread-key="blog/07252016/Essentials-of-Machine-Learning-Algorithms-with-Python-and-R-Codes-FW/" data-title="Essentials of Machine Learning Algorithms with Python and R Codes (FW)" data-url="http://www.aprilzephyr.com/blog/07252016/Essentials-of-Machine-Learning-Algorithms-with-Python-and-R-Codes-FW/"></div>
<script type="text/javascript">
var duoshuoQuery = {short_name:"duoshuo_name"};
(function() {
	var ds = document.createElement('script');
	ds.type = 'text/javascript';ds.async = true;
	ds.src = (document.location.protocol == 'https:' ? 'https:' : 'http:') + '//static.duoshuo.com/embed.js';
	ds.charset = 'UTF-8';
	(document.getElementsByTagName('head')[0] 
	 || document.getElementsByTagName('body')[0]).appendChild(ds);
})();
</script>
</div>
    </div>
</section>


      
<!-- ============================ Footer =========================== -->

<footer>
    <div class="container">
            <div class="copy">
                <p>
                    &copy; 2014<script>new Date().getFullYear()>2010&&document.write("-"+new Date().getFullYear());</script>, Content By Themis_Sword. All Rights Reserved.
                </p>
                <p>Theme By <a href="//go.kieran.top" style="color: #767D84">Kieran.</a> Thanks!</p>
            </div>
<div align='right'>
    <form class="navbar-form" action="/search/">
        <input type="text" class="form-control" placeholder="Google Search" name="q">
    </form>
 </div>
            
            
            <div class="social">
                <ul>
                    
                    
                    
                    
                    
                </ul>
            </div>
            <div class="clearfix"> </div>
        </div>
</footer>

<!-- ============================ END Footer =========================== -->
      <!-- Load our scripts -->
        
<!-- Resizable 'on-demand' full-height hero -->
<script type="text/javascript">
    
    var resizeHero = function () {
        var hero = $(".cover,.heightblock"),
            window1 = $(window);
        hero.css({
            "height": window1.height()
        });
    };
    
    resizeHero();
    
    $(window).resize(function () {
        resizeHero();
    });
</script>
<script src="/js/plugins.min.js"></script><!-- Bootstrap core and concatenated plugins always load here -->
<script src="/js/jquery.flexslider-min.js"></script><!-- Flexslider plugin -->
<script src="/js/scripts.js"></script><!-- Theme scripts -->

<!-- Initiate flexslider plugin -->
<script type="text/javascript">
    $(document).ready(function($) {
      $('.flexslider').flexslider({
        animation: "fade",
        prevText: "",
        nextText: "",
        directionNav: true
      });
    });
</script>

</body>
</html>
